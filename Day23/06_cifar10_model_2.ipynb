{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">注</font>: 使用 tensorboard 可视化需要安装 tensorflow (TensorBoard依赖于tensorflow库，可以任意安装tensorflow的gpu/cpu版本)\n",
    "\n",
    "```shell\n",
    "pip install tensorflow-cpu\n",
    "```"
   ],
   "id": "5bf63ff71ff56691"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T09:41:11.733585Z",
     "start_time": "2025-02-06T09:40:57.632142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "    \n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n"
   ],
   "id": "e911a585d198bee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.0\n",
      "numpy 2.0.2\n",
      "pandas 2.2.3\n",
      "sklearn 1.6.0\n",
      "torch 2.5.1+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据准备\n",
    "\n",
    "```shell\n",
    "$ tree -L 1 cifar-10                                    \n",
    "cifar-10\n",
    "├── sampleSubmission.csv\n",
    "├── test\n",
    "├── train\n",
    "└── trainLabels.csv\n",
    "```"
   ],
   "id": "32c93af88db42b67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:42:10.053336Z",
     "start_time": "2025-02-06T10:42:07.874328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path  # 导入Path类，用于处理文件路径\n",
    "\n",
    "# 定义数据目录路径\n",
    "DATA_DIR = Path(\"E:/BaiduNetdiskDownload/cifar-10\")\n",
    "\n",
    "# 定义训练标签文件路径\n",
    "train_lables_file = DATA_DIR / \"trainLabels.csv\"\n",
    "# 定义测试集模板CSV文件路径\n",
    "test_csv_file = DATA_DIR / \"sampleSubmission.csv\"\n",
    "# 定义训练集图片文件夹路径\n",
    "train_folder = DATA_DIR / \"train\"\n",
    "# 定义测试集图片文件夹路径\n",
    "test_folder = DATA_DIR / \"test\"\n",
    "\n",
    "# 所有的类别名称\n",
    "class_names = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck',\n",
    "]\n",
    "\n",
    "def parse_csv_file(filepath, folder):\n",
    "    \"\"\"解析CSV文件，将其转换为包含文件路径和标签的元组列表\"\"\"\n",
    "    results = []  # 初始化结果列表\n",
    "    with open(filepath, 'r') as f:  # 以只读模式打开CSV文件\n",
    "        lines = f.readlines()[1:]  # 读取所有行，并去掉第一行（标题行）\n",
    "    for line in lines:  # 遍历每一行\n",
    "        image_id, label_str = line.strip('\\n').split(',')  # 去除换行符并按逗号分割，获取图片ID和标签\n",
    "        image_full_path = folder / f\"{image_id}.png\"  # 构建图片的完整路径\n",
    "        results.append((image_full_path, label_str))  # 将图片路径和标签作为元组添加到结果列表\n",
    "    return results  # 返回结果列表\n",
    "\n",
    "# 解析训练集标签文件\n",
    "train_labels_info = parse_csv_file(train_lables_file, train_folder)\n",
    "# 解析测试集CSV文件\n",
    "test_csv_info = parse_csv_file(test_csv_file, test_folder)\n",
    "\n",
    "# 打印训练集和测试集的前5条信息\n",
    "import pprint\n",
    "pprint.pprint(train_labels_info[0:5])\n",
    "pprint.pprint(test_csv_info[0:5])\n",
    "\n",
    "# 打印训练集和测试集的总长度\n",
    "print(len(train_labels_info), len(test_csv_info))\n"
   ],
   "id": "2755100121ea2323",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(WindowsPath('E:/BaiduNetdiskDownload/cifar-10/train/1.png'), 'frog'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/train/2.png'), 'truck'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/train/3.png'), 'truck'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/train/4.png'), 'deer'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/train/5.png'), 'automobile')]\n",
      "[(WindowsPath('E:/BaiduNetdiskDownload/cifar-10/test/1.png'), 'cat'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/test/2.png'), 'cat'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/test/3.png'), 'cat'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/test/4.png'), 'cat'),\n",
      " (WindowsPath('E:/BaiduNetdiskDownload/cifar-10/test/5.png'), 'cat')]\n",
      "50000 300000\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:42:16.227157Z",
     "start_time": "2025-02-06T10:42:16.130762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_df = pd.DataFrame(train_labels_info)\n",
    "train_df = pd.DataFrame(train_labels_info[0:45000]) # 取前45000张图片作为训练集\n",
    "valid_df = pd.DataFrame(train_labels_info[45000:]) # 取后5000张图片作为验证集\n",
    "test_df = pd.DataFrame(test_csv_info)\n",
    "\n",
    "train_df.columns = ['filepath', 'class']\n",
    "valid_df.columns = ['filepath', 'class']\n",
    "test_df.columns = ['filepath', 'class']\n",
    "\n",
    "print(train_df.head())\n",
    "print(valid_df.head())\n",
    "print(test_df.head())"
   ],
   "id": "63d7abda2488094e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       filepath       class\n",
      "0  E:\\BaiduNetdiskDownload\\cifar-10\\train\\1.png        frog\n",
      "1  E:\\BaiduNetdiskDownload\\cifar-10\\train\\2.png       truck\n",
      "2  E:\\BaiduNetdiskDownload\\cifar-10\\train\\3.png       truck\n",
      "3  E:\\BaiduNetdiskDownload\\cifar-10\\train\\4.png        deer\n",
      "4  E:\\BaiduNetdiskDownload\\cifar-10\\train\\5.png  automobile\n",
      "                                           filepath       class\n",
      "0  E:\\BaiduNetdiskDownload\\cifar-10\\train\\45001.png       horse\n",
      "1  E:\\BaiduNetdiskDownload\\cifar-10\\train\\45002.png  automobile\n",
      "2  E:\\BaiduNetdiskDownload\\cifar-10\\train\\45003.png        deer\n",
      "3  E:\\BaiduNetdiskDownload\\cifar-10\\train\\45004.png  automobile\n",
      "4  E:\\BaiduNetdiskDownload\\cifar-10\\train\\45005.png    airplane\n",
      "                                      filepath class\n",
      "0  E:\\BaiduNetdiskDownload\\cifar-10\\test\\1.png   cat\n",
      "1  E:\\BaiduNetdiskDownload\\cifar-10\\test\\2.png   cat\n",
      "2  E:\\BaiduNetdiskDownload\\cifar-10\\test\\3.png   cat\n",
      "3  E:\\BaiduNetdiskDownload\\cifar-10\\test\\4.png   cat\n",
      "4  E:\\BaiduNetdiskDownload\\cifar-10\\test\\5.png   cat\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:43:08.319934Z",
     "start_time": "2025-02-06T10:43:06.083453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image  # 导入PIL库中的Image模块，用于图像处理\n",
    "from torch.utils.data import Dataset, DataLoader  # 从PyTorch的torch.utils.data模块导入Dataset和DataLoader类，用于数据集的处理和加载\n",
    "from torchvision import transforms  # 从torchvision库导入transforms模块，用于图像预处理和数据增强\n",
    "\n",
    "class Cifar10Dataset(Dataset):\n",
    "    # 定义一个字典，将模式映射到对应的数据集\n",
    "    df_map = {\n",
    "        \"train\": train_df,  # 训练集数据\n",
    "        \"eval\": valid_df,   # 验证集数据\n",
    "        \"test\": test_df     # 测试集数据\n",
    "    }\n",
    "    # 定义类别到索引的映射字典\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(class_names)}\n",
    "    # 定义索引到类别的映射字典\n",
    "    idx_to_label = {idx: label for idx, label in enumerate(class_names)}\n",
    "\n",
    "    def __init__(self, mode, transform=None):\n",
    "        \"\"\"\n",
    "        初始化Cifar10Dataset类的实例\n",
    "        :param mode: 数据集模式，'train'、'eval'或'test'\n",
    "        :param transform: 可选的图像预处理或数据增强操作\n",
    "        \"\"\"\n",
    "        self.df = self.df_map.get(mode, None)  # 根据模式获取对应的数据集\n",
    "        if self.df is None:\n",
    "            raise ValueError(f\"mode should be one of train, eval, test, but got {mode}\")\n",
    "        self.transform = transform  # 存储传入的预处理或数据增强操作\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        获取指定索引的样本\n",
    "        :param index: 样本索引\n",
    "        :return: 处理后的图像和对应的标签\n",
    "        \"\"\"\n",
    "        img_path, label = self.df.iloc[index]  # 获取图像路径和标签\n",
    "        img = Image.open(img_path).convert('RGB')  # 打开图像并转换为RGB模式\n",
    "        if self.transform:\n",
    "            img = self.transform(img)  # 如果定义了预处理或数据增强操作，则应用之\n",
    "        label = self.label_to_idx[label]  # 将标签转换为对应的索引\n",
    "        return img, label  # 返回处理后的图像和标签索引\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        获取数据集的大小\n",
    "        :return: 数据集中的样本数量\n",
    "        \"\"\"\n",
    "        return len(self.df)  # 返回数据集的样本数量\n",
    "\n",
    "# 定义图像的目标尺寸\n",
    "IMAGE_SIZE = 32\n",
    "# 定义图像的均值和标准差，用于标准化\n",
    "mean, std = [0.4914, 0.4822, 0.4465], [0.247, 0.243, 0.261]\n",
    "\n",
    "# 定义训练集的预处理和数据增强操作\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # 调整图像大小\n",
    "    transforms.RandomRotation(40),  # 随机旋转图像\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平翻转图像\n",
    "    transforms.ToTensor(),  # 将图像转换为Tensor\n",
    "    # transforms.Normalize(mean, std)  # 标准化图像（此行被注释掉）\n",
    "])\n",
    "\n",
    "# 定义验证集和测试集的预处理操作\n",
    "transforms_eval = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # 调整图像大小\n",
    "    transforms.ToTensor(),  # 将图像转换为Tensor\n",
    "    transforms.Normalize(mean, std)  # 标准化图像\n",
    "])\n",
    "\n",
    "# 创建训练集数据集实例\n",
    "train_ds = Cifar10Dataset(\"train\", transforms_train)\n",
    "# 创建验证集数据集实例\n",
    "eval_ds = Cifar10Dataset(\"eval\", transforms_eval)\n"
   ],
   "id": "8d1bbad28f9a3629",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:43:12.387332Z",
     "start_time": "2025-02-06T10:43:12.309035Z"
    }
   },
   "cell_type": "code",
   "source": "train_ds[0][0].shape # 图片的shape,输入",
   "id": "97e1564b479e10ca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:43:14.067593Z",
     "start_time": "2025-02-06T10:43:14.062064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_ds.idx_to_label)  # 类别映射为idx\n",
    "train_ds.label_to_idx # idx映射为类别"
   ],
   "id": "10363c7cae7d1512",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:43:17.005363Z",
     "start_time": "2025-02-06T10:43:17.001742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)   \n",
    "eval_dl = DataLoader(eval_ds, batch_size=batch_size, shuffle=False)"
   ],
   "id": "6b6a684c2b104e84",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T10:48:14.440271Z",
     "start_time": "2025-02-06T10:43:25.931521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 遍历train_ds得到每张图片，计算每个通道的均值和方差\n",
    "def cal_mean_std(ds):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for img, _ in ds:\n",
    "        mean += img.mean(dim=(1, 2))\n",
    "        std += img.std(dim=(1, 2))\n",
    "    mean /= len(ds)\n",
    "    std /= len(ds)\n",
    "    return mean, std\n",
    "\n",
    "# 经过 normalize 后 均值为0，方差为1\n",
    "print(cal_mean_std(train_ds))"
   ],
   "id": "438a08124a01d2e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.4369, 0.4268, 0.3947]), tensor([0.2465, 0.2420, 0.2360]))\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "6d2071cf4dd3e49a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T11:51:37.180576Z",
     "start_time": "2025-02-06T11:51:37.047172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义一个卷积神经网络 (CNN) 类，继承自 nn.Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        # 调用父类构造函数\n",
    "        super().__init__()\n",
    "\n",
    "        # 定义网络的模型结构\n",
    "        self.model = nn.Sequential(\n",
    "            # 第一层卷积层：输入通道数3（RGB图片），输出通道数128，卷积核大小3x3，padding=\"same\"保持输入输出的大小一致\n",
    "            nn.Conv2d(in_channels=3, out_channels=128, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),  # 激活函数ReLU，增加非线性\n",
    "            nn.BatchNorm2d(128),  # 批标准化，用于对卷积结果进行归一化，减少训练时的内部协变量偏移\n",
    "            \n",
    "            # 第二层卷积层：输入通道数128，输出通道数128，卷积核大小3x3，padding=\"same\"\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=\"same\"), \n",
    "            nn.ReLU(),  # 激活函数ReLU\n",
    "            nn.BatchNorm2d(128),  # 批标准化\n",
    "\n",
    "            # 最大池化层：kernel_size=2，表示池化窗口大小为2x2，步长为2，输出尺寸减半\n",
    "            nn.MaxPool2d(kernel_size=2),  # 输出尺寸 (128, 16, 16)\n",
    "\n",
    "            # 第三层卷积层：输入通道数128，输出通道数256，卷积核大小3x3，padding=\"same\"\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),  # 激活函数ReLU\n",
    "            nn.BatchNorm2d(256),  # 批标准化\n",
    "\n",
    "            # 第四层卷积层：输入通道数256，输出通道数256，卷积核大小3x3，padding=\"same\"\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),  # 激活函数ReLU\n",
    "            nn.BatchNorm2d(256),  # 批标准化\n",
    "\n",
    "            # 最大池化层：kernel_size=2，表示池化窗口大小为2x2，步长为2，输出尺寸减半\n",
    "            nn.MaxPool2d(kernel_size=2),  # 输出尺寸 (256, 8, 8)\n",
    "\n",
    "            # 第五层卷积层：输入通道数256，输出通道数512，卷积核大小3x3，padding=\"same\"\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),  # 激活函数ReLU\n",
    "            nn.BatchNorm2d(512),  # 批标准化\n",
    "\n",
    "            # 第六层卷积层：输入通道数512，输出通道数512，卷积核大小3x3，padding=\"same\"\n",
    "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=\"same\"),\n",
    "            nn.ReLU(),  # 激活函数ReLU\n",
    "            nn.BatchNorm2d(512),  # 批标准化\n",
    "\n",
    "            # 最大池化层：kernel_size=2，表示池化窗口大小为2x2，步长为2，输出尺寸减半\n",
    "            nn.MaxPool2d(kernel_size=2),  # 输出尺寸 (512, 4, 4)\n",
    "\n",
    "            # 将多维的特征图展平成一维向量，准备输入全连接层\n",
    "            nn.Flatten(),  # 展平\n",
    "\n",
    "            # 第一层全连接层：输入特征为4*4*512=8192，输出特征数为512\n",
    "            nn.Linear(8192, 512),\n",
    "            nn.ReLU(),  # 激活函数ReLU\n",
    "\n",
    "            # 第二层全连接层：输入特征数为512，输出为num_classes（类别数），对应最终分类任务\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    # 前向传播函数，定义数据如何流经模型\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 打印模型的每个参数的名称和参数数量\n",
    "for key, value in CNN(len(class_names)).named_parameters():\n",
    "    print(f\"{key:^40}paramerters num: {np.prod(value.shape)}\")\n"
   ],
   "id": "f71188e90c8ca67f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             model.0.weight             paramerters num: 3456\n",
      "              model.0.bias              paramerters num: 128\n",
      "             model.2.weight             paramerters num: 128\n",
      "              model.2.bias              paramerters num: 128\n",
      "             model.3.weight             paramerters num: 147456\n",
      "              model.3.bias              paramerters num: 128\n",
      "             model.5.weight             paramerters num: 128\n",
      "              model.5.bias              paramerters num: 128\n",
      "             model.7.weight             paramerters num: 294912\n",
      "              model.7.bias              paramerters num: 256\n",
      "             model.9.weight             paramerters num: 256\n",
      "              model.9.bias              paramerters num: 256\n",
      "            model.10.weight             paramerters num: 589824\n",
      "             model.10.bias              paramerters num: 256\n",
      "            model.12.weight             paramerters num: 256\n",
      "             model.12.bias              paramerters num: 256\n",
      "            model.14.weight             paramerters num: 1179648\n",
      "             model.14.bias              paramerters num: 512\n",
      "            model.16.weight             paramerters num: 512\n",
      "             model.16.bias              paramerters num: 512\n",
      "            model.17.weight             paramerters num: 2359296\n",
      "             model.17.bias              paramerters num: 512\n",
      "            model.19.weight             paramerters num: 512\n",
      "             model.19.bias              paramerters num: 512\n",
      "            model.22.weight             paramerters num: 4194304\n",
      "             model.22.bias              paramerters num: 512\n",
      "            model.24.weight             paramerters num: 5120\n",
      "             model.24.bias              paramerters num: 10\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:04:16.146914Z",
     "start_time": "2025-02-06T12:04:16.096912Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in CNN(len(class_names)).parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ],
   "id": "a074c033ded7f007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 8779914\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:04:18.251136Z",
     "start_time": "2025-02-06T12:04:18.220179Z"
    }
   },
   "cell_type": "code",
   "source": "512*4*4",
   "id": "48ac02929c39ede2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:04:21.181822Z",
     "start_time": "2025-02-06T12:04:21.031668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_4d = torch.randn(32, 3, 64, 64)  # 32 个样本，3 个通道，图像大小为 64x64\n",
    "bn2d = nn.BatchNorm2d(3)              # 对 3 个通道进行归一化\n",
    "output_4d = bn2d(input_4d)\n",
    "output_4d.shape"
   ],
   "id": "8f0c372e3214ea3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:13:10.567935Z",
     "start_time": "2025-02-06T12:13:10.544128Z"
    }
   },
   "cell_type": "code",
   "source": "output_4d",
   "id": "23e272a79692849",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0220e+00,  3.4402e-01, -4.4450e-01,  ...,  1.4650e+00,\n",
       "            1.0845e-01,  1.6322e+00],\n",
       "          [ 7.9437e-01, -5.8831e-01, -8.5833e-01,  ..., -5.0367e-01,\n",
       "           -4.7779e-01,  4.9912e-01],\n",
       "          [-2.2494e+00,  1.0997e+00,  1.5307e+00,  ..., -6.4616e-01,\n",
       "           -4.3801e-01,  8.3926e-01],\n",
       "          ...,\n",
       "          [ 5.2208e-02,  1.2663e+00,  6.7109e-01,  ..., -1.0686e+00,\n",
       "            4.4737e-01,  1.0966e+00],\n",
       "          [-1.2508e+00, -8.2019e-02,  1.3695e+00,  ...,  7.1213e-01,\n",
       "           -8.1538e-01, -6.8265e-01],\n",
       "          [-2.4799e-01, -3.6358e-01,  1.5454e-01,  ..., -1.1900e+00,\n",
       "           -1.6888e+00, -9.0077e-01]],\n",
       "\n",
       "         [[-1.2966e+00,  1.3186e+00, -2.0587e+00,  ...,  4.0129e-02,\n",
       "           -1.3070e+00,  1.1656e+00],\n",
       "          [-1.5719e-01, -1.4689e+00, -1.0365e+00,  ...,  2.0640e+00,\n",
       "           -6.7039e-01, -4.2742e-01],\n",
       "          [-9.0844e-01, -1.3823e+00,  1.2226e+00,  ..., -5.6536e-01,\n",
       "           -1.1478e-01, -3.7578e-01],\n",
       "          ...,\n",
       "          [ 2.8787e-01, -1.2344e+00,  1.1127e+00,  ...,  4.6050e-02,\n",
       "            5.2099e-01, -6.0737e-01],\n",
       "          [ 1.1458e-02, -5.9725e-02,  1.4047e+00,  ...,  1.5711e-01,\n",
       "            1.1226e+00,  2.0294e+00],\n",
       "          [-1.4716e+00,  3.5328e-01, -2.2716e+00,  ...,  2.7800e-01,\n",
       "            7.1230e-02, -1.0624e-01]],\n",
       "\n",
       "         [[-1.4123e+00,  4.7209e-01,  3.6362e-01,  ...,  6.2698e-01,\n",
       "           -2.5197e-01, -8.6391e-01],\n",
       "          [ 2.3834e-01, -3.5268e-01, -1.1360e+00,  ...,  5.1096e-01,\n",
       "            7.3330e-01, -1.4579e+00],\n",
       "          [-2.9183e-01,  4.0515e-01, -2.7556e-02,  ...,  1.6303e+00,\n",
       "            1.3823e-01,  1.2528e+00],\n",
       "          ...,\n",
       "          [-1.2842e-01,  8.9316e-01, -7.7801e-01,  ..., -3.4371e-01,\n",
       "            1.4110e+00,  9.2494e-01],\n",
       "          [-4.6257e-01,  3.6536e-01, -8.5515e-01,  ...,  1.1920e+00,\n",
       "           -2.2766e-01, -1.7439e+00],\n",
       "          [-2.0994e-02, -3.0606e-01, -5.8680e-01,  ...,  4.4170e-01,\n",
       "            1.3440e-01, -8.1722e-02]]],\n",
       "\n",
       "\n",
       "        [[[-4.5160e-01, -6.2213e-01, -4.2442e-01,  ..., -1.3023e+00,\n",
       "           -1.0611e+00, -9.1430e-01],\n",
       "          [ 9.7564e-02, -2.1890e+00, -3.8042e-01,  ..., -7.8324e-01,\n",
       "           -1.4822e+00,  1.4461e-01],\n",
       "          [ 3.5911e-01,  2.1138e-01,  1.9337e+00,  ..., -6.0019e-02,\n",
       "           -2.0994e-01,  6.2475e-01],\n",
       "          ...,\n",
       "          [ 1.4630e+00, -2.2503e+00,  7.3333e-01,  ...,  1.1931e+00,\n",
       "            4.9842e-01, -1.1973e-01],\n",
       "          [ 1.3181e+00,  4.2293e-01,  1.6649e+00,  ..., -3.6652e-02,\n",
       "           -7.8520e-01, -1.0072e+00],\n",
       "          [-1.3880e+00,  1.2997e+00, -1.3212e+00,  ...,  6.0750e-01,\n",
       "            6.3040e-01,  5.8617e-01]],\n",
       "\n",
       "         [[-2.1759e+00, -5.4194e-01, -3.4809e-01,  ..., -4.6050e-01,\n",
       "            9.4132e-02,  8.7025e-02],\n",
       "          [-7.0545e-02, -1.2896e+00,  5.5504e-01,  ..., -8.3355e-01,\n",
       "            2.4025e+00, -1.4125e+00],\n",
       "          [-8.0319e-01, -1.4336e+00,  1.2325e+00,  ..., -7.2936e-01,\n",
       "           -4.9670e-01, -1.6326e-01],\n",
       "          ...,\n",
       "          [ 2.8932e-01, -7.6670e-01, -9.4628e-01,  ...,  9.6418e-01,\n",
       "            5.9127e-01, -4.9694e-01],\n",
       "          [-1.6222e-01, -7.4086e-01, -3.4019e-01,  ..., -7.1177e-01,\n",
       "           -7.2901e-02, -1.2170e+00],\n",
       "          [ 1.0488e+00,  7.1455e-01, -1.5655e+00,  ..., -3.7551e-02,\n",
       "            2.7487e-01, -1.2576e+00]],\n",
       "\n",
       "         [[-2.9144e+00,  1.2205e+00,  1.2132e+00,  ...,  6.5944e-01,\n",
       "            7.2759e-02,  4.8796e-01],\n",
       "          [-6.8082e-01,  3.0649e-01, -8.3379e-01,  ...,  2.1161e-01,\n",
       "           -1.3871e-02,  1.3066e+00],\n",
       "          [ 1.6587e+00, -2.3485e+00, -4.7131e-01,  ..., -1.7013e+00,\n",
       "            4.1520e-01,  3.5891e-01],\n",
       "          ...,\n",
       "          [-3.9061e+00, -6.3389e-01,  5.5965e-01,  ..., -4.4086e-01,\n",
       "            3.5787e-01,  1.9908e-01],\n",
       "          [ 4.9266e-01,  1.5585e+00, -2.3549e-01,  ...,  6.7182e-01,\n",
       "            4.8429e-01, -1.7615e-01],\n",
       "          [ 3.3023e-01,  5.0532e-01,  1.8114e-01,  ..., -6.2879e-01,\n",
       "           -7.7772e-01,  6.8963e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.3459e-01, -7.5804e-01,  1.3830e-01,  ..., -1.1670e+00,\n",
       "           -1.5761e+00, -8.9846e-01],\n",
       "          [-3.0548e-02,  1.7240e+00,  2.0707e+00,  ..., -1.9090e-01,\n",
       "            8.7100e-02,  9.8018e-01],\n",
       "          [ 4.3627e-01,  8.7250e-02,  5.0067e-01,  ..., -2.0152e+00,\n",
       "            2.7606e-01, -1.5554e+00],\n",
       "          ...,\n",
       "          [-4.1680e-01, -1.1168e+00,  3.4078e-01,  ..., -5.2497e-01,\n",
       "           -8.3259e-01,  1.3880e-01],\n",
       "          [-9.7075e-01, -7.2557e-01, -1.1819e+00,  ..., -1.8468e+00,\n",
       "            8.0841e-01, -7.5882e-01],\n",
       "          [ 1.0560e+00,  1.4105e+00, -1.1664e-01,  ...,  2.3949e-01,\n",
       "            7.1085e-01,  9.1154e-01]],\n",
       "\n",
       "         [[-2.4820e-01, -2.0158e-01, -8.9336e-01,  ...,  1.3639e-01,\n",
       "           -1.5049e+00,  2.1532e-01],\n",
       "          [ 1.6293e+00,  1.8277e+00, -6.8057e-01,  ..., -9.9158e-01,\n",
       "           -1.2082e+00, -8.7685e-01],\n",
       "          [-1.2217e-01, -1.0517e+00,  1.9568e+00,  ...,  1.3758e-01,\n",
       "            7.3674e-01,  6.2533e-02],\n",
       "          ...,\n",
       "          [ 1.3426e-01, -6.6939e-01, -7.8505e-01,  ..., -9.2097e-01,\n",
       "            4.9471e-01, -2.5344e-01],\n",
       "          [ 1.2456e+00,  1.9911e+00, -7.0898e-01,  ...,  3.2751e-01,\n",
       "            5.6314e-01, -1.9716e-01],\n",
       "          [-3.2438e-01,  4.3154e-01,  4.7544e-01,  ...,  2.9897e+00,\n",
       "           -1.0650e+00,  8.5452e-01]],\n",
       "\n",
       "         [[-1.4130e+00,  6.1954e-02,  6.4458e-01,  ..., -1.4419e+00,\n",
       "            8.9497e-01, -8.2183e-01],\n",
       "          [ 9.0317e-02, -1.0579e-01, -9.3688e-01,  ..., -6.6718e-01,\n",
       "            1.9838e-01,  2.1229e-01],\n",
       "          [ 5.9334e-01,  1.7889e-02, -8.5344e-01,  ...,  1.8291e+00,\n",
       "            1.9172e+00,  1.2005e+00],\n",
       "          ...,\n",
       "          [ 4.3207e-01,  1.8836e-01,  7.6678e-01,  ...,  1.5957e-01,\n",
       "            6.2332e-01, -1.1103e-01],\n",
       "          [ 1.4483e+00,  1.0425e+00, -3.3586e-01,  ..., -6.6255e-01,\n",
       "           -7.1865e-01, -6.1175e-01],\n",
       "          [ 8.0173e-01, -1.1586e+00,  9.0155e-01,  ..., -2.0355e+00,\n",
       "            3.0095e-01, -1.1803e+00]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-7.5027e-01,  8.1497e-02,  1.8933e-01,  ..., -6.9975e-01,\n",
       "           -1.6348e+00,  1.0202e+00],\n",
       "          [ 2.0211e+00, -7.2893e-01,  4.5566e-01,  ...,  3.0231e-01,\n",
       "           -1.4224e+00,  5.0937e-01],\n",
       "          [-3.9820e-01, -2.9237e-01,  1.8344e-01,  ..., -2.7508e+00,\n",
       "           -1.6840e-01, -1.3730e+00],\n",
       "          ...,\n",
       "          [ 2.5296e-01, -2.8012e-01,  3.8953e-01,  ...,  5.1009e-01,\n",
       "            4.4634e-03, -7.3919e-01],\n",
       "          [-1.3016e+00,  5.0533e-01,  4.1321e-01,  ...,  2.3302e-01,\n",
       "            1.9207e-01, -3.1916e-02],\n",
       "          [-1.6477e+00,  5.9755e-01, -2.0624e-01,  ...,  1.3440e+00,\n",
       "           -2.3189e-01,  5.5555e-01]],\n",
       "\n",
       "         [[ 7.8138e-03,  1.3539e+00,  2.5399e-01,  ..., -3.7073e-01,\n",
       "            2.9792e+00,  1.1837e+00],\n",
       "          [-1.1649e+00, -1.0669e+00,  8.6898e-01,  ..., -3.0746e+00,\n",
       "            1.8332e-02, -1.6258e-02],\n",
       "          [ 1.1140e+00,  6.9584e-01, -2.9255e-01,  ...,  9.4732e-01,\n",
       "            6.6277e-01,  9.6832e-01],\n",
       "          ...,\n",
       "          [-1.0987e+00,  1.5803e-01,  9.1676e-01,  ..., -7.5549e-01,\n",
       "           -1.0308e+00,  1.2114e+00],\n",
       "          [-5.5458e-01, -1.4928e-01, -1.4554e+00,  ...,  2.4667e-02,\n",
       "            5.1065e-01,  6.2130e-01],\n",
       "          [ 5.9841e-01, -1.5160e+00, -8.0789e-01,  ...,  3.2252e-01,\n",
       "           -4.3485e-01,  1.4956e-01]],\n",
       "\n",
       "         [[ 9.1540e-01,  9.8965e-01,  7.0811e-01,  ..., -3.2697e-01,\n",
       "           -9.9798e-01,  7.1510e-01],\n",
       "          [-3.4581e-01, -1.2620e-01, -6.5872e-01,  ...,  2.2360e+00,\n",
       "            2.9311e-03, -1.7721e+00],\n",
       "          [-4.0286e-01,  1.5638e-01,  1.3012e-01,  ..., -1.9977e+00,\n",
       "            9.2732e-01, -4.5514e-01],\n",
       "          ...,\n",
       "          [-1.7682e+00, -1.8385e+00,  6.2932e-01,  ...,  9.5014e-01,\n",
       "           -1.1534e-02, -8.0724e-01],\n",
       "          [-1.1552e-01,  1.3370e+00, -5.5111e-01,  ...,  5.1075e-01,\n",
       "           -1.4393e+00,  8.4232e-02],\n",
       "          [ 2.4275e+00,  1.8617e+00, -6.1386e-01,  ..., -9.5211e-02,\n",
       "            1.5530e+00, -9.8998e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0650e+00,  3.5016e-01,  1.0159e+00,  ...,  6.1750e-01,\n",
       "            2.4016e-01, -6.1037e-01],\n",
       "          [ 1.0945e+00,  1.0092e+00, -2.0709e+00,  ...,  1.2588e+00,\n",
       "            6.7906e-01, -4.1727e-01],\n",
       "          [ 8.7277e-01,  1.0213e+00, -1.2143e+00,  ..., -9.5028e-01,\n",
       "            1.3889e+00,  1.9542e+00],\n",
       "          ...,\n",
       "          [-2.3228e+00,  1.3736e-01, -6.9349e-01,  ...,  7.1418e-01,\n",
       "            9.9977e-01,  1.7168e+00],\n",
       "          [ 6.2467e-02, -2.0812e-01,  6.2146e-01,  ..., -5.4021e-01,\n",
       "           -1.9195e+00,  3.3435e-02],\n",
       "          [ 5.3434e-01, -3.1278e-01,  2.6582e+00,  ..., -9.4609e-01,\n",
       "           -3.0584e-01,  3.3715e-02]],\n",
       "\n",
       "         [[ 1.7751e-01, -1.6299e+00, -1.6779e+00,  ...,  1.5036e+00,\n",
       "            1.1198e+00, -5.1835e-01],\n",
       "          [ 1.4826e+00,  3.2059e-01,  2.6388e-01,  ...,  1.0819e+00,\n",
       "           -5.3029e-01, -3.3718e-01],\n",
       "          [ 3.3111e-01, -2.8487e-01,  3.7614e-01,  ..., -3.6762e-01,\n",
       "            3.0777e-01,  1.6430e+00],\n",
       "          ...,\n",
       "          [-4.4182e-01, -8.8091e-01, -1.2347e+00,  ..., -8.7073e-01,\n",
       "           -5.3518e-01, -3.6545e-01],\n",
       "          [ 1.5172e+00,  1.7340e+00,  1.7862e+00,  ..., -6.3892e-01,\n",
       "           -2.1737e-01, -1.3386e+00],\n",
       "          [ 2.0321e+00, -1.2741e-01,  5.6129e-01,  ..., -2.8938e-01,\n",
       "            2.4334e-01,  3.8744e-01]],\n",
       "\n",
       "         [[ 4.2139e-01, -2.6543e+00,  2.3261e-01,  ..., -9.4012e-01,\n",
       "           -9.4511e-01,  1.7608e+00],\n",
       "          [-1.5057e+00, -1.8881e-01, -3.8720e-01,  ...,  1.6955e-01,\n",
       "            5.8284e-01, -5.8531e-01],\n",
       "          [ 6.3683e-01,  5.9427e-01,  1.9317e-02,  ...,  3.5155e-01,\n",
       "           -3.0037e-01, -1.3680e+00],\n",
       "          ...,\n",
       "          [-1.0871e+00, -7.8501e-01,  6.9029e-01,  ..., -2.0564e+00,\n",
       "           -5.8479e-01, -2.9757e-01],\n",
       "          [-1.0029e+00,  1.4586e-01,  1.2764e+00,  ...,  6.3502e-01,\n",
       "            2.0024e+00,  1.5052e+00],\n",
       "          [ 1.0728e+00, -1.2357e+00, -5.6527e-02,  ...,  1.3205e+00,\n",
       "            6.0554e-01,  6.9607e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.8766e-01, -1.2993e+00, -2.5546e-01,  ..., -6.9066e-01,\n",
       "           -2.1718e-01,  3.7947e-01],\n",
       "          [ 1.2088e+00,  3.7491e-02,  5.5115e-01,  ..., -1.0613e-01,\n",
       "            7.4432e-01,  1.7894e+00],\n",
       "          [-1.1684e+00, -5.0904e-01, -1.2137e+00,  ...,  7.5221e-01,\n",
       "           -5.6900e-02, -1.0873e+00],\n",
       "          ...,\n",
       "          [ 5.1165e-01, -1.1234e+00, -6.5934e-01,  ...,  8.4021e-01,\n",
       "            7.1050e-02,  1.0497e+00],\n",
       "          [-8.3510e-01, -9.6678e-01,  5.6673e-01,  ..., -2.3393e-01,\n",
       "           -2.6666e+00, -2.0712e-01],\n",
       "          [ 5.6393e-01, -5.3193e-01,  7.9761e-02,  ...,  7.3741e-01,\n",
       "           -2.5459e-01, -7.0621e-01]],\n",
       "\n",
       "         [[ 1.7036e+00, -7.9351e-01,  9.0521e-01,  ..., -1.0063e+00,\n",
       "            1.2034e+00, -1.0168e+00],\n",
       "          [-2.0080e+00, -2.3066e-01, -1.0945e+00,  ...,  3.5332e-01,\n",
       "           -1.4469e-01, -4.5907e-01],\n",
       "          [-2.5842e-02, -9.4820e-01, -4.2379e-01,  ...,  8.6813e-01,\n",
       "           -8.3337e-01, -7.3954e-02],\n",
       "          ...,\n",
       "          [-7.0963e-01,  2.0484e-01,  6.9229e-01,  ..., -1.3306e+00,\n",
       "            1.2864e+00,  8.5430e-01],\n",
       "          [ 3.4314e-01,  7.6486e-01,  8.1823e-01,  ...,  1.1646e+00,\n",
       "           -3.0369e-01,  3.3836e-01],\n",
       "          [-9.3829e-01,  4.6617e-01, -3.6287e+00,  ...,  1.1501e+00,\n",
       "           -8.9884e-01, -7.4235e-01]],\n",
       "\n",
       "         [[ 5.6846e-01, -1.4151e-01, -6.2427e-01,  ...,  1.2855e+00,\n",
       "            3.9223e-01,  1.6241e+00],\n",
       "          [-1.8574e-01, -1.7944e+00,  7.6898e-01,  ...,  3.6190e-01,\n",
       "            8.0548e-02, -1.5154e-02],\n",
       "          [-6.6944e-01, -5.0732e-01, -4.3015e-01,  ..., -2.5283e-01,\n",
       "           -9.7575e-01, -9.1861e-02],\n",
       "          ...,\n",
       "          [-1.1471e+00, -7.5116e-01,  7.9181e-01,  ..., -6.1881e-01,\n",
       "           -1.5238e+00, -8.4311e-01],\n",
       "          [-2.7436e+00,  9.3528e-01,  1.0006e+00,  ..., -2.0731e+00,\n",
       "           -1.3700e+00, -7.2665e-01],\n",
       "          [-4.1131e-01, -1.2423e+00,  1.0960e+00,  ...,  4.9793e-01,\n",
       "            2.1622e-01,  3.4949e-01]]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:13:19.041808Z",
     "start_time": "2025-02-06T12:13:19.017398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bn2d = nn.BatchNorm2d(1)              # 对 3 个通道进行归一化\n",
    "output_4d1 = bn2d(input_4d[:, 0:1, :, :])\n",
    "output_4d1"
   ],
   "id": "ae5ceca0d1d9dfa9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.0220,  0.3440, -0.4445,  ...,  1.4650,  0.1084,  1.6322],\n",
       "          [ 0.7944, -0.5883, -0.8583,  ..., -0.5037, -0.4778,  0.4991],\n",
       "          [-2.2494,  1.0997,  1.5307,  ..., -0.6462, -0.4380,  0.8393],\n",
       "          ...,\n",
       "          [ 0.0522,  1.2663,  0.6711,  ..., -1.0686,  0.4474,  1.0966],\n",
       "          [-1.2508, -0.0820,  1.3695,  ...,  0.7121, -0.8154, -0.6827],\n",
       "          [-0.2480, -0.3636,  0.1545,  ..., -1.1900, -1.6888, -0.9008]]],\n",
       "\n",
       "\n",
       "        [[[-0.4516, -0.6221, -0.4244,  ..., -1.3023, -1.0611, -0.9143],\n",
       "          [ 0.0976, -2.1890, -0.3804,  ..., -0.7832, -1.4822,  0.1446],\n",
       "          [ 0.3591,  0.2114,  1.9337,  ..., -0.0600, -0.2099,  0.6248],\n",
       "          ...,\n",
       "          [ 1.4630, -2.2503,  0.7333,  ...,  1.1931,  0.4984, -0.1197],\n",
       "          [ 1.3181,  0.4229,  1.6649,  ..., -0.0367, -0.7852, -1.0072],\n",
       "          [-1.3880,  1.2997, -1.3212,  ...,  0.6075,  0.6304,  0.5862]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8346, -0.7580,  0.1383,  ..., -1.1670, -1.5761, -0.8985],\n",
       "          [-0.0305,  1.7240,  2.0707,  ..., -0.1909,  0.0871,  0.9802],\n",
       "          [ 0.4363,  0.0873,  0.5007,  ..., -2.0152,  0.2761, -1.5554],\n",
       "          ...,\n",
       "          [-0.4168, -1.1168,  0.3408,  ..., -0.5250, -0.8326,  0.1388],\n",
       "          [-0.9707, -0.7256, -1.1819,  ..., -1.8468,  0.8084, -0.7588],\n",
       "          [ 1.0560,  1.4105, -0.1166,  ...,  0.2395,  0.7108,  0.9115]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.7503,  0.0815,  0.1893,  ..., -0.6997, -1.6348,  1.0202],\n",
       "          [ 2.0211, -0.7289,  0.4557,  ...,  0.3023, -1.4224,  0.5094],\n",
       "          [-0.3982, -0.2924,  0.1834,  ..., -2.7508, -0.1684, -1.3730],\n",
       "          ...,\n",
       "          [ 0.2530, -0.2801,  0.3895,  ...,  0.5101,  0.0045, -0.7392],\n",
       "          [-1.3016,  0.5053,  0.4132,  ...,  0.2330,  0.1921, -0.0319],\n",
       "          [-1.6477,  0.5976, -0.2062,  ...,  1.3440, -0.2319,  0.5556]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0650,  0.3502,  1.0159,  ...,  0.6175,  0.2402, -0.6104],\n",
       "          [ 1.0945,  1.0092, -2.0709,  ...,  1.2588,  0.6791, -0.4173],\n",
       "          [ 0.8728,  1.0213, -1.2143,  ..., -0.9503,  1.3889,  1.9542],\n",
       "          ...,\n",
       "          [-2.3228,  0.1374, -0.6935,  ...,  0.7142,  0.9998,  1.7168],\n",
       "          [ 0.0625, -0.2081,  0.6215,  ..., -0.5402, -1.9195,  0.0334],\n",
       "          [ 0.5343, -0.3128,  2.6582,  ..., -0.9461, -0.3058,  0.0337]]],\n",
       "\n",
       "\n",
       "        [[[-0.1877, -1.2993, -0.2555,  ..., -0.6907, -0.2172,  0.3795],\n",
       "          [ 1.2088,  0.0375,  0.5511,  ..., -0.1061,  0.7443,  1.7894],\n",
       "          [-1.1684, -0.5090, -1.2137,  ...,  0.7522, -0.0569, -1.0873],\n",
       "          ...,\n",
       "          [ 0.5116, -1.1234, -0.6593,  ...,  0.8402,  0.0710,  1.0497],\n",
       "          [-0.8351, -0.9668,  0.5667,  ..., -0.2339, -2.6666, -0.2071],\n",
       "          [ 0.5639, -0.5319,  0.0798,  ...,  0.7374, -0.2546, -0.7062]]]],\n",
       "       grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 训练\n",
    "\n",
    "pytorch的训练需要自行实现，包括\n",
    "1. 定义损失函数\n",
    "2. 定义优化器\n",
    "3. 定义训练步\n",
    "4. 训练"
   ],
   "id": "38467863420fa215"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:14:24.034865Z",
     "start_time": "2025-02-06T12:14:23.731555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    loss_list = []\n",
    "    pred_list = []\n",
    "    label_list = []\n",
    "    for datas, labels in dataloader:\n",
    "        datas = datas.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 前向计算\n",
    "        logits = model(datas)\n",
    "        loss = loss_fct(logits, labels)         # 验证集损失\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        preds = logits.argmax(axis=-1)    # 验证集预测\n",
    "        pred_list.extend(preds.cpu().numpy().tolist())\n",
    "        label_list.extend(labels.cpu().numpy().tolist())\n",
    "        \n",
    "    acc = accuracy_score(label_list, pred_list)\n",
    "    return np.mean(loss_list), acc\n"
   ],
   "id": "3e1bfb6cf3858d63",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### TensorBoard 可视化\n",
    "\n",
    "\n",
    "训练过程中可以使用如下命令启动tensorboard服务。\n",
    "\n",
    "```shell\n",
    "tensorboard \\\n",
    "    --logdir=runs \\     # log 存放路径\n",
    "    --host 0.0.0.0 \\    # ip\n",
    "    --port 8848         # 端口\n",
    "```"
   ],
   "id": "b4b0ecbbfd9011e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:14:53.024640Z",
     "start_time": "2025-02-06T12:14:45.201827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TensorBoardCallback:\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_dir (str): dir to write log.\n",
    "            flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs)\n",
    "\n",
    "    def draw_model(self, model, input_shape):\n",
    "        self.writer.add_graph(model, input_to_model=torch.randn(input_shape))\n",
    "        \n",
    "    def add_loss_scalars(self, step, loss, val_loss):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/loss\", \n",
    "            tag_scalar_dict={\"loss\": loss, \"val_loss\": val_loss},\n",
    "            global_step=step,\n",
    "            )\n",
    "        \n",
    "    def add_acc_scalars(self, step, acc, val_acc):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/accuracy\",\n",
    "            tag_scalar_dict={\"accuracy\": acc, \"val_accuracy\": val_acc},\n",
    "            global_step=step,\n",
    "        )\n",
    "        \n",
    "    def add_lr_scalars(self, step, learning_rate):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/learning_rate\",\n",
    "            tag_scalar_dict={\"learning_rate\": learning_rate},\n",
    "            global_step=step,\n",
    "            \n",
    "        )\n",
    "    \n",
    "    def __call__(self, step, **kwargs):\n",
    "        # add loss\n",
    "        loss = kwargs.pop(\"loss\", None)\n",
    "        val_loss = kwargs.pop(\"val_loss\", None)\n",
    "        if loss is not None and val_loss is not None:\n",
    "            self.add_loss_scalars(step, loss, val_loss)\n",
    "        # add acc\n",
    "        acc = kwargs.pop(\"acc\", None)\n",
    "        val_acc = kwargs.pop(\"val_acc\", None)\n",
    "        if acc is not None and val_acc is not None:\n",
    "            self.add_acc_scalars(step, acc, val_acc)\n",
    "        # add lr\n",
    "        learning_rate = kwargs.pop(\"lr\", None)\n",
    "        if learning_rate is not None:\n",
    "            self.add_lr_scalars(step, learning_rate)\n"
   ],
   "id": "2ed79a0d5226f5d5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Best\n",
   "id": "b8f8acfc6d9f291c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:15:04.417829Z",
     "start_time": "2025-02-06T12:15:04.412344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SaveCheckpointsCallback:\n",
    "    def __init__(self, save_dir, save_step=5000, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Save checkpoints each save_epoch epoch. \n",
    "        We save checkpoint by epoch in this implementation.\n",
    "        Usually, training scripts with pytorch evaluating model and save checkpoint by step.\n",
    "\n",
    "        Args:\n",
    "            save_dir (str): dir to save checkpoint\n",
    "            save_epoch (int, optional): the frequency to save checkpoint. Defaults to 1.\n",
    "            save_best_only (bool, optional): If True, only save the best model or save each model at every epoch.\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        self.save_step = save_step\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_metrics = -1\n",
    "        \n",
    "        # mkdir\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "        \n",
    "    def __call__(self, step, state_dict, metric=None):\n",
    "        if step % self.save_step > 0:\n",
    "            return\n",
    "        \n",
    "        if self.save_best_only:\n",
    "            assert metric is not None\n",
    "            if metric >= self.best_metrics:\n",
    "                # save checkpoints\n",
    "                torch.save(state_dict, os.path.join(self.save_dir, \"best.ckpt\"))\n",
    "                # update best metrics\n",
    "                self.best_metrics = metric\n",
    "        else:\n",
    "            torch.save(state_dict, os.path.join(self.save_dir, f\"{step}.ckpt\"))\n",
    "\n"
   ],
   "id": "5617ea12a297073b",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Early Stop",
   "id": "ac72e616297e8414"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:15:19.142531Z",
     "start_time": "2025-02-06T12:15:19.137157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            patience (int, optional): Number of epochs with no improvement after which training will be stopped.. Defaults to 5.\n",
    "            min_delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute \n",
    "                change of less than min_delta, will count as no improvement. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = -1\n",
    "        self.counter = 0\n",
    "        \n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            # update best metric\n",
    "            self.best_metric = metric\n",
    "            # reset counter \n",
    "            self.counter = 0\n",
    "        else: \n",
    "            self.counter += 1\n",
    "            \n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ],
   "id": "6ad12481cf4eadc4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:15:55.924378Z",
     "start_time": "2025-02-06T12:15:55.028357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 训练\n",
    "def training(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    epoch, \n",
    "    loss_fct, \n",
    "    optimizer, \n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "    \n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_loader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            # training\n",
    "            for datas, labels in train_loader:\n",
    "                datas = datas.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 梯度清空\n",
    "                optimizer.zero_grad()\n",
    "                # 模型前向计算\n",
    "                logits = model(datas)\n",
    "                # 计算损失\n",
    "                loss = loss_fct(logits, labels)\n",
    "                # 梯度回传\n",
    "                loss.backward()\n",
    "                # 调整优化器，包括学习率的变动等\n",
    "                optimizer.step()\n",
    "                preds = logits.argmax(axis=-1) #最大值的索引\n",
    "            \n",
    "                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())     # 计算准确率\n",
    "                loss = loss.cpu().item() # 计算损失\n",
    "                # record\n",
    "                \n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"acc\": acc, \"step\": global_step # 记录每一步的损失和准确率\n",
    "                })\n",
    "                \n",
    "                # evaluating\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)\n",
    "                    record_dict[\"val\"].append({\n",
    "                        \"loss\": val_loss, \"acc\": val_acc, \"step\": global_step\n",
    "                    })\n",
    "                    model.train()\n",
    "                    \n",
    "                    # 1. 使用 tensorboard 可视化\n",
    "                    if tensorboard_callback is not None:\n",
    "                        tensorboard_callback(\n",
    "                            global_step, \n",
    "                            loss=loss, val_loss=val_loss,\n",
    "                            acc=acc, val_acc=val_acc,\n",
    "                            lr=optimizer.param_groups[0][\"lr\"],\n",
    "                            )\n",
    "                \n",
    "                    # 2. 保存模型权重 save model checkpoint\n",
    "                    if save_ckpt_callback is not None:\n",
    "                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc)\n",
    "\n",
    "                    # 3. 早停 Early Stop\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(val_acc)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "                    \n",
    "                # udate step\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"epoch\": epoch_id})\n",
    "        \n",
    "    return record_dict\n",
    "        \n",
    "\n",
    "epoch = 20\n",
    "\n",
    "model = CNN(num_classes=10)\n",
    "\n",
    "# 1. 定义损失函数 采用交叉熵损失\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "# 2. 定义优化器 采用 adam\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 1. tensorboard 可视化\n",
    "if not os.path.exists(\"runs\"):\n",
    "    os.mkdir(\"runs\")\n",
    "tensorboard_callback = TensorBoardCallback(\"runs/cifar-10\")\n",
    "tensorboard_callback.draw_model(model, [1, 3, IMAGE_SIZE, IMAGE_SIZE])\n",
    "# 2. save best\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "save_ckpt_callback = SaveCheckpointsCallback(\"checkpoints/cifar-10\", save_step=len(train_dl), save_best_only=True)\n",
    "# 3. early stop\n",
    "early_stop_callback = EarlyStopCallback(patience=5)\n",
    "\n",
    "model = model.to(device)\n"
   ],
   "id": "80946c2c6d4da0ff",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "record = training(\n",
    "    model,\n",
    "    train_dl,\n",
    "    eval_dl,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=save_ckpt_callback,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    eval_step=len(train_dl)\n",
    "    )"
   ],
   "id": "8b0559361029da79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#画线要注意的是损失是不一定在零到1之间的\n",
    "def plot_learning_curves(record_dict, sample_step=500):\n",
    "    # build DataFrame\n",
    "    train_df = pd.DataFrame(record_dict[\"train\"]).set_index(\"step\").iloc[::sample_step]\n",
    "    val_df = pd.DataFrame(record_dict[\"val\"]).set_index(\"step\")\n",
    "\n",
    "    # plot\n",
    "    fig_num = len(train_df.columns)\n",
    "    fig, axs = plt.subplots(1, fig_num, figsize=(5 * fig_num, 5))\n",
    "    for idx, item in enumerate(train_df.columns):    \n",
    "        axs[idx].plot(train_df.index, train_df[item], label=f\"train_{item}\")\n",
    "        axs[idx].plot(val_df.index, val_df[item], label=f\"val_{item}\")\n",
    "        axs[idx].grid()\n",
    "        axs[idx].legend()\n",
    "        # axs[idx].set_xticks(range(0, train_df.index[-1], 5000))\n",
    "        # axs[idx].set_xticklabels(map(lambda x: f\"{int(x/1000)}k\", range(0, train_df.index[-1], 5000)))\n",
    "        axs[idx].set_xlabel(\"step\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(record, sample_step=10)  #横坐标是 steps"
   ],
   "id": "2043eee391227d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# dataload for evaluating\n",
    "\n",
    "# load checkpoints\n",
    "model.load_state_dict(torch.load(\"checkpoints/cifar-10/best.ckpt\", map_location=\"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "loss, acc = evaluating(model, eval_dl, loss_fct)\n",
    "print(f\"loss:     {loss:.4f}\\naccuracy: {acc:.4f}\")"
   ],
   "id": "4e59c4d6179b59e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 评估",
   "id": "818a49f031b36ad2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T12:17:20.127822Z",
     "start_time": "2025-02-06T12:17:19.252843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dataload for evaluating\n",
    "\n",
    "# load checkpoints\n",
    "model.load_state_dict(torch.load(\"checkpoints/cifar-10/best.ckpt\", map_location=\"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "loss, acc = evaluating(model, eval_dl, loss_fct)\n",
    "print(f\"loss:     {loss:.4f}\\naccuracy: {acc:.4f}\")"
   ],
   "id": "db6973c2b22a213a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\14984\\AppData\\Local\\Temp\\ipykernel_37020\\1513600467.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"checkpoints/cifar-10/best.ckpt\", map_location=\"cpu\"))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/cifar-10/best.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# dataload for evaluating\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# load checkpoints\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m model\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcheckpoints/cifar-10/best.ckpt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m      6\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m      7\u001B[0m loss, acc \u001B[38;5;241m=\u001B[39m evaluating(model, eval_dl, loss_fct)\n",
      "File \u001B[1;32mD:\\develop_tools\\dev-tools-pyhton\\python312\\Lib\\site-packages\\torch\\serialization.py:1319\u001B[0m, in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m   1317\u001B[0m     pickle_load_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1319\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[0;32m   1320\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[0;32m   1321\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[0;32m   1322\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[0;32m   1323\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[0;32m   1324\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[1;32mD:\\develop_tools\\dev-tools-pyhton\\python312\\Lib\\site-packages\\torch\\serialization.py:659\u001B[0m, in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    657\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[0;32m    658\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[1;32m--> 659\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    660\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    661\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[1;32mD:\\develop_tools\\dev-tools-pyhton\\python312\\Lib\\site-packages\\torch\\serialization.py:640\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[1;32m--> 640\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'checkpoints/cifar-10/best.ckpt'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 推理",
   "id": "94478d9a1e017c38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# test_df\n",
    "test_ds = Cifar10Dataset(\"test\", transform=transforms_eval)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "preds_collect = [] # 预测结果收集器\n",
    "model.eval()\n",
    "for data, fake_label in tqdm(test_dl):\n",
    "    data = data.to(device=device)\n",
    "    logits = model(data) #得到预测结果\n",
    "    preds = [test_ds.idx_to_label[idx] for idx in logits.argmax(axis=-1).cpu().tolist()] # 得到预测类别，idx_to_label是id到字符串类别的映射\n",
    "    preds_collect.extend(preds)\n",
    "    \n",
    "test_df[\"label\"] = preds_collect # 增加预测类别列,比赛要求这一列是label\n",
    "test_df.head()"
   ],
   "id": "d1cc342747511594"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "64*4688",
   "id": "de87849eb3496d7c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 导出 submission.csv\n",
    "test_df.to_csv(\"submission.csv\", index=False)"
   ],
   "id": "2a2356302e71d6bb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
