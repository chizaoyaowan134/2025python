{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">注</font>: 使用 tensorboard 可视化需要安装 tensorflow (TensorBoard依赖于tensorflow库，可以任意安装tensorflow的gpu/cpu版本)\n",
    "\n",
    "```shell\n",
    "pip install tensorflow-cpu\n",
    "```"
   ],
   "id": "148a2ff50eb1b54d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:17:55.307534Z",
     "start_time": "2025-02-10T02:17:42.503517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, torch:\n",
    "    print(module.__name__, module.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "seed = 42\n"
   ],
   "id": "a1ba91883bf0cb52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=12, micro=3, releaselevel='final', serial=0)\n",
      "matplotlib 3.10.0\n",
      "numpy 2.0.2\n",
      "pandas 2.2.3\n",
      "sklearn 1.6.0\n",
      "torch 2.5.1+cpu\n",
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:19:06.644114Z",
     "start_time": "2025-02-10T02:18:59.122150Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install kaggle",
   "id": "7b432ddc6a3f2d25",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting kaggle\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/66/e3/c775e2213bac0ac1f1fd601d1c915d13934c5944cbff153ede6584acab50/kaggle-1.6.17.tar.gz (82 kB)\n",
      "     ---------------------------------------- 0.0/82.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 82.7/82.7 kB 4.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six>=1.10 in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (2024.12.14)\n",
      "Requirement already satisfied: python-dateutil in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (4.67.1)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a4/62/02da182e544a51a5c3ccf4b03ab79df279f9c60c5e82d5e8bec7ca26ac11/python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: urllib3 in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (2.3.0)\n",
      "Requirement already satisfied: bleach in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: webencodings in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/a6/a5/c0b6468d3824fe3fde30dbb5e1f687b291608f9473681bbf7dabbf5a87d7/text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.2/78.2 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from requests->kaggle) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from requests->kaggle) (3.10)\n",
      "Requirement already satisfied: colorama in d:\\develop_tools\\dev-tools-pyhton\\python312\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py): started\n",
      "  Building wheel for kaggle (setup.py): finished with status 'done'\n",
      "  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105797 sha256=820786dcb87b5da5a1a0ef2665123dc8011919061c5dcf3c75c14e525bf254b1\n",
      "  Stored in directory: c:\\users\\14984\\appdata\\local\\pip\\cache\\wheels\\8d\\56\\22\\6d8d9d828c29ab865663199ed38e2daf198575df40bae91329\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:19:18.745462Z",
     "start_time": "2025-02-10T02:19:18.628129Z"
    }
   },
   "cell_type": "code",
   "source": "!pwd",
   "id": "7f7c4f211adb68fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/python_code/2025python/Day24\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!rm -r /content/datasets/",
   "id": "4be403c2ac8944f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 导入Python内置的json模块，该模块提供了处理JSON数据的方法\n",
    "import json\n",
    "\n",
    "# 定义一个包含Kaggle账号凭证的Python字典\n",
    "# 通常用于API身份验证，其中：\n",
    "# - \"username\"：Kaggle平台注册的用户名\n",
    "# - \"key\"：Kaggle账户的API密钥（用于命令行工具认证）\n",
    "token = {\"username\":\"chizaoyaowan123\",\"key\":\"5a374c8b00ade92b48c3c99f5c96a58b\"}\n",
    "\n",
    "# 使用with语句安全地操作文件：\n",
    "# 1. 打开/mnt/workspace/kaggle.json文件，'w'表示写入模式（如果文件存在会被覆盖）\n",
    "# 2. 将文件对象赋值给变量file\n",
    "# （文件路径说明：通常在Linux系统中，/mnt/用于挂载点，workspace可能是工作目录）\n",
    "with open('/mnt/workspace/kaggle.json', 'w') as file:\n",
    "    # 使用json.dump()方法将token字典序列化为JSON格式\n",
    "    # 并写入到已打开的文件对象中\n",
    "    # 最终生成标准JSON格式文件，包含用户名和API密钥\n",
    "    json.dump(token, file)"
   ],
   "id": "887f1b39c43995c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!cat /mnt/workspace/kaggle.json",
   "id": "6353f1abb3350e17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. 创建一个名为 .kaggle 的文件夹到用户的主目录中\n",
    "#    - 使用 \"-p\" 参数，即使该目录已经存在也不会报错，同时会自动创建任何不存在的父级目录\n",
    "!mkdir -p ~/.kaggle\n",
    "\n",
    "# 2. 将存放在 /mnt/workspace/ 下的 kaggle.json 文件复制到刚刚创建的 .kaggle 目录中\n",
    "#    - kaggle.json 文件通常包含了你的 Kaggle API 密钥和配置信息\n",
    "!cp /mnt/workspace/kaggle.json ~/.kaggle/\n",
    "\n",
    "# 3. 修改 kaggle.json 文件的权限，设置为 600\n",
    "#    - 权限 600 表示只有文件所有者具有读写权限，这样可以保证 API 密钥的安全，防止其他用户读取\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# 4. 使用 Kaggle 命令行工具设置配置项\n",
    "#    - 此处设置 \"path\" 配置项的值为 /content，表示将来通过 Kaggle 工具下载的数据或文件默认存储到 /content 目录\n",
    "!kaggle config set -n path -v /content\n"
   ],
   "id": "9a11bda2cc28bd86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!kaggle datasets download -d slothkong/10-monkey-species",
   "id": "103a291195cace97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 这段命令主要是用于在 Linux 或类似 Unix 的环境中解压一个 zip 文件，并将解压后的内容放置在指定目录，同时覆盖已存在的同名文件，且不显示任何输出\n",
    "!unzip -o -d /content /content/datasets/slothkong/10-monkey-species/10-monkey-species.zip >/dev/null "
   ],
   "id": "62e0b46edcb738ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据准备\n",
    "\n",
    "```shell\n",
    "$ tree -L 2 archive\n",
    "archive\n",
    "├── monkey_labels.txt\n",
    "├── training\n",
    "│   ├── n0\n",
    "│   ├── n1\n",
    "│   ├── n2\n",
    "│   ├── n3\n",
    "│   ├── n4\n",
    "│   ├── n5\n",
    "│   ├── n6\n",
    "│   ├── n7\n",
    "│   ├── n8\n",
    "│   └── n9\n",
    "└── validation\n",
    "    ├── n0\n",
    "    ├── n1\n",
    "    ├── n2\n",
    "    ├── n3\n",
    "    ├── n4\n",
    "    ├── n5\n",
    "    ├── n6\n",
    "    ├── n7\n",
    "    ├── n8\n",
    "    └── n9\n",
    "\n",
    "22 directories, 1 file\n",
    "```"
   ],
   "id": "55a1fb4c52832088"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-10T02:27:23.795955Z",
     "start_time": "2025-02-10T02:27:23.662597Z"
    }
   },
   "cell_type": "code",
   "source": "!ls",
   "id": "3ae578d172ed2886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04_10_monkeys_model_1_aliyun.ipynb\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 从 torchvision 包中导入 datasets 模块，用于处理数据集相关操作\n",
    "from torchvision import datasets\n",
    "# 从 torchvision.transforms 中导入多种数据预处理操作：\n",
    "# ToTensor：将图片转换为张量格式；Resize：调整图片尺寸；\n",
    "# Compose：将多个转换操作组合；ConvertImageDtype：转换张量的数据类型；\n",
    "# Normalize：对图片进行归一化处理\n",
    "from torchvision.transforms import ToTensor, Resize, Compose, ConvertImageDtype, Normalize\n",
    "\n",
    "# 从 pathlib 模块中导入 Path 类，用于方便地操作文件路径\n",
    "from pathlib import Path\n",
    "\n",
    "# 定义两个数据目录常量：\n",
    "# DATA_DIR1 指向训练数据存放的根目录，通常在云平台或本地文件系统中指定路径\n",
    "DATA_DIR1 = Path(\"/content/training/\")\n",
    "# DATA_DIR2 指向验证数据存放的根目录\n",
    "DATA_DIR2 = Path(\"/content/validation/\")\n",
    "\n",
    "# 定义一个自定义数据集类 MonkeyDataset，该类继承自 torchvision.datasets.ImageFolder\n",
    "# ImageFolder 是一个便捷的类，可以自动将文件夹中按子文件夹存放的图片视为不同类别进行加载\n",
    "class MonkeyDataset(datasets.ImageFolder):\n",
    "    # 定义构造函数，参数 mode 指定加载哪种数据（训练集或验证集），transform 为预处理操作\n",
    "    def __init__(self, mode, transform=None):\n",
    "        # 根据 mode 参数判断需要加载的数据类型\n",
    "        if mode == \"train\":\n",
    "            # 如果 mode 为 \"train\"，则构造训练集的路径\n",
    "            # 注意：此处 DATA_DIR1 / \"training\" 表示在 DATA_DIR1 路径下的 \"training\" 子文件夹\n",
    "            root = DATA_DIR1 / \"training\"\n",
    "        elif mode == \"val\":\n",
    "            # 如果 mode 为 \"val\"，则构造验证集的路径\n",
    "            # 同理，DATA_DIR2 / \"validation\" 表示在 DATA_DIR2 路径下的 \"validation\" 子文件夹\n",
    "            root = DATA_DIR2 / \"validation\"\n",
    "        else:\n",
    "            # 如果 mode 参数既不是 \"train\" 也不是 \"val\"，则抛出异常，提醒用户 mode 参数输入错误\n",
    "            raise ValueError(\"mode should be one of the following: train, val, but got {}\".format(mode))\n",
    "        # 调用父类 ImageFolder 的构造函数，将确定好的根目录 root 和传入的 transform 参数传递给父类\n",
    "        super().__init__(root, transform) # 调用父类init方法\n",
    "        # 将父类 ImageFolder 解析后的 samples 赋值给 self.imgs\n",
    "        # self.samples 是一个列表，每个元素为 (图片路径, 标签) 的元组，便于后续处理\n",
    "        self.imgs = self.samples\n",
    "        # 从 samples 中提取所有标签，生成一个标签列表，存放于 self.targets 中\n",
    "        # 通过列表解析提取每个样本元组中第二个元素（标签）\n",
    "        self.targets = [s[1] for s in self.samples] # 标签取出来\n",
    "\n",
    "# 定义预处理过程中目标图片的尺寸，这里将图片统一调整为 128 像素的高度和 128 像素的宽度\n",
    "img_h, img_w = 128, 128\n",
    "\n",
    "# 定义一个复合的图片预处理操作 transform，通过 Compose 将多个转换操作依次组合：\n",
    "transform = Compose([\n",
    "    # Resize 操作：将输入图片调整为 (img_h, img_w) 指定的尺寸\n",
    "    Resize((img_h, img_w)), # 图片缩放\n",
    "    # ToTensor 操作：将图片转换为 PyTorch 张量，并自动将图片像素值归一化到 [0, 1] 的范围内\n",
    "    ToTensor(),\n",
    "    # Normalize 操作：使用给定的均值和标准差对图片进行归一化\n",
    "    # 此处的均值 [0.4363, 0.4328, 0.3291] 和标准差 [0.2085, 0.2032, 0.1988] 是预先统计得到的\n",
    "    Normalize([0.4363, 0.4328, 0.3291], [0.2085, 0.2032, 0.1988]),\n",
    "    # ConvertImageDtype 操作：将图片张量的数据类型转换为 torch.float\n",
    "    # 注意：此处需要确保已导入 torch 库，否则可能会报错；不过由于不修改代码，这里仅作说明\n",
    "    ConvertImageDtype(torch.float), # 转换为float类型\n",
    "]) # 数据预处理的组合\n",
    "\n",
    "# 根据上面自定义的 MonkeyDataset 类，创建训练集数据对象\n",
    "# 参数 \"train\" 指定加载训练集，同时传入预处理操作 transform\n",
    "train_ds = MonkeyDataset(\"train\", transform=transform)\n",
    "# 同理，创建验证集数据对象，参数 \"val\" 指定加载验证集\n",
    "val_ds = MonkeyDataset(\"val\", transform=transform)\n",
    "\n",
    "# 打印训练集加载的图片数量，通过 len(train_ds) 获取图片总数\n",
    "print(\"load {} images from training dataset\".format(len(train_ds)))\n",
    "# 打印验证集加载的图片数量，通过 len(val_ds) 获取图片总数\n",
    "print(\"load {} images from validation dataset\".format(len(val_ds)))\n"
   ],
   "id": "eb88798fb239b4ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 数据类别\n",
    "train_ds.classes"
   ],
   "id": "83fa6c00d33b8481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "train_ds.class_to_idx",
   "id": "de51e3045601ef26"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 图片路径 及 标签\n",
    "for fpath, label in train_ds.imgs:\n",
    "    print(fpath, label)\n",
    "    break\n",
    "\n",
    "#这个和之前的dataset完全一致\n",
    "for img, label in train_ds:\n",
    "    # c, h, w  label\n",
    "    print(img, label)\n",
    "    break"
   ],
   "id": "93616231c284238d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 这里是是对三个通道的均值方差进行统计，分别是R、G、B通道的均值方差",
   "id": "7935115038efb44a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cal_mean_std(ds):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for img, _ in ds:\n",
    "        mean += img[0:1, :, :].mean(dim=(1, 2))\n",
    "        std += img[0:1, :, :].std(dim=(1, 2))\n",
    "    mean /= len(ds)\n",
    "    std /= len(ds)\n",
    "    return mean, std\n",
    "\n",
    "# 经过 normalize 后 均值为0，方差为1\n",
    "print(cal_mean_std(train_ds))"
   ],
   "id": "1e48fef17e93896d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cal_mean_std(ds):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for img, _ in ds:\n",
    "        mean += img[1:2, :, :].mean(dim=(1, 2))\n",
    "        std += img[1:2, :, :].std(dim=(1, 2))\n",
    "    mean /= len(ds)\n",
    "    std /= len(ds)\n",
    "    return mean, std\n",
    "\n",
    "# 经过 normalize 后 均值为0，方差为1\n",
    "print(cal_mean_std(train_ds))"
   ],
   "id": "6bb65fb15881c337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cal_mean_std(ds):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for img, _ in ds:\n",
    "        mean += img[2:3, :, :].mean(dim=(1, 2))\n",
    "        std += img[2:3, :, :].std(dim=(1, 2))\n",
    "    mean /= len(ds)\n",
    "    std /= len(ds)\n",
    "    return mean, std\n",
    "\n",
    "# 经过 normalize 后 均值为0，方差为1\n",
    "print(cal_mean_std(train_ds))"
   ],
   "id": "7121010c97fdc0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 遍历train_ds得到每张图片，计算每个通道的均值和方差\n",
    "def cal_mean_std(ds):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    for img, _ in ds:\n",
    "        mean += img.mean(dim=(1, 2)) #dim=(1, 2)表示计算均值后，宽和高消除（把宽和高所有的像素加起来，再除以总数）\n",
    "        std += img.std(dim=(1, 2))\n",
    "    mean /= len(ds)\n",
    "    std /= len(ds)\n",
    "    return mean, std\n",
    "\n",
    "# 经过 normalize 后 均值为0，方差为1\n",
    "# print(cal_mean_std(train_ds))"
   ],
   "id": "2e4655a10a9df0f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "# 从数据集到dataloader\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4) #训练集\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=4) #验证集"
   ],
   "id": "f8e786d149426443"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 定义模型",
   "id": "fae03fc08a4f78f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义一个卷积神经网络模型，继承自 PyTorch 的 nn.Module\n",
    "class CNN(nn.Module):\n",
    "    # 构造函数，num_classes 表示分类的类别数，activation 用于选择激活函数（\"relu\" 或 \"selu\"）\n",
    "    def __init__(self, num_classes=10, activation=\"relu\"):\n",
    "        super(CNN, self).__init__()\n",
    "        # 根据 activation 参数选择激活函数\n",
    "        # 如果 activation == \"relu\"，则使用 F.relu，否则使用 F.selu\n",
    "        self.activation = F.relu if activation == \"relu\" else F.selu\n",
    "\n",
    "        # 定义第一层卷积层：\n",
    "        # 输入通道数为3（例如RGB图像），输出通道数为32，卷积核大小为3x3\n",
    "        # padding=\"same\" 表示边缘填充，使得输出的宽高与输入保持一致\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "        # 定义第二层卷积层：\n",
    "        # 输入通道数为32（上一层输出），输出通道数仍为32，卷积核同样为3x3，padding=\"same\"\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "        # 定义池化层：\n",
    "        # 使用最大池化，窗口大小为2x2，步长为2，将特征图的宽和高各缩小一半\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # 定义第三层卷积层：\n",
    "        # 输入通道数为32，输出通道数为64，卷积核大小为3x3，padding=\"same\"\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "        # 定义第四层卷积层：\n",
    "        # 输入通道数为64，输出通道数仍为64，卷积核大小为3x3，padding=\"same\"\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "        # 定义第五层卷积层：\n",
    "        # 输入通道数为64，输出通道数为128，卷积核大小为3x3，padding=\"same\"\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "        # 定义第六层卷积层：\n",
    "        # 输入通道数为128，输出通道数仍为128，卷积核大小为3x3，padding=\"same\"\n",
    "        self.conv6 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=\"same\")\n",
    "        \n",
    "        # 定义展平层，将多维张量展平成一维向量，便于后续全连接层处理\n",
    "        self.flatten = nn.Flatten()\n",
    "        # 定义第一个全连接层 fc1：\n",
    "        # 经过三次池化后，假设输入图像尺寸为 (3, 128, 128)，尺寸会变成 (128, 16, 16)\n",
    "        # 所以展平后的特征数为 128 * 16 * 16，映射到 128 维特征\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 128)\n",
    "        \n",
    "        # 定义第二个全连接层 fc2：\n",
    "        # 将 fc1 输出的 128 维特征映射到最终分类的类别数（num_classes）\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        # 初始化模型中所有全连接层和卷积层的权重\n",
    "        self.init_weights()\n",
    "\n",
    "    # 定义权重初始化函数，使用 Xavier 均匀分布来初始化权重\n",
    "    def init_weights(self):\n",
    "        \"\"\"使用 xavier 均匀分布来初始化全连接层、卷积层的权重 W\"\"\"\n",
    "        # 遍历模型的所有子模块\n",
    "        for m in self.modules():\n",
    "            # 判断当前子模块是否为全连接层或卷积层\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                # 使用 Xavier 均匀分布初始化权重，能使得网络更容易训练和收敛\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                # 将偏置初始化为 0\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    # 定义前向传播函数，描述输入数据如何经过各层处理得到输出\n",
    "    def forward(self, x):\n",
    "        # 简写激活函数为 act，便于后续调用\n",
    "        act = self.activation\n",
    "        \n",
    "        # 第一组卷积操作：\n",
    "        # 依次经过 conv1 -> 激活 -> conv2 -> 激活，再经过最大池化层 pool\n",
    "        # 这一组操作会提取低级特征，并将特征图尺寸缩小一半\n",
    "        x = self.pool(act(self.conv2(act(self.conv1(x)))))\n",
    "        \n",
    "        # 第二组卷积操作：\n",
    "        # 使用 conv3 和 conv4 进行两次卷积操作，每次卷积后均经过激活函数，最后再进行池化\n",
    "        x = self.pool(act(self.conv4(act(self.conv3(x)))))\n",
    "        \n",
    "        # 第三组卷积操作：\n",
    "        # 使用 conv5 和 conv6 进行两次卷积操作，激活后进行池化，进一步提取更高级的特征\n",
    "        x = self.pool(act(self.conv6(act(self.conv5(x)))))\n",
    "        \n",
    "        # 将卷积层输出的多维张量展平成一维向量\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # 第一个全连接层 fc1，将展平后的特征映射到 128 维特征，并经过激活函数\n",
    "        x = act(self.fc1(x))\n",
    "        \n",
    "        # 第二个全连接层 fc2，将特征映射到最终分类结果（类别数）\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # 返回最终输出结果\n",
    "        return x\n",
    "\n",
    "# 遍历模型的所有参数，打印每个参数的名称及其参数数量\n",
    "# 例如，卷积层和全连接层中的权重和偏置参数的数量\n",
    "for idx, (key, value) in enumerate(CNN().named_parameters()):\n",
    "    print(f\"{key}\\tparamerters num: {np.prod(value.shape)}\")\n"
   ],
   "id": "1d3c026fcf8dc1f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 训练\n",
    "\n",
    "pytorch的训练需要自行实现，包括\n",
    "1. 定义损失函数\n",
    "2. 定义优化器\n",
    "3. 定义训练步\n",
    "4. 训练"
   ],
   "id": "c0af4f7c418a929c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 从 scikit-learn 库中导入 accuracy_score 函数，用于计算分类准确率\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 使用 @torch.no_grad() 装饰器，表示在该函数内部不需要计算梯度，\n",
    "# 这在验证或测试时可以节省内存和计算资源，避免梯度追踪\n",
    "@torch.no_grad()\n",
    "def evaluating(model, dataloader, loss_fct):\n",
    "    # 初始化一个列表，用于存储每个 batch 计算得到的损失值\n",
    "    loss_list = []\n",
    "    # 初始化一个列表，用于存储所有 batch 的预测标签\n",
    "    pred_list = []\n",
    "    # 初始化一个列表，用于存储所有 batch 的真实标签\n",
    "    label_list = []\n",
    "    \n",
    "    # 遍历 dataloader 中的所有数据，dataloader 会按 batch 返回数据和对应的标签\n",
    "    for datas, labels in dataloader:\n",
    "        # 将输入数据 datas 移动到指定设备上（例如 GPU 或 CPU），device 需在其他地方定义\n",
    "        datas = datas.to(device)\n",
    "        # 同样，将标签数据也移动到相同设备上，确保数据和标签在同一设备上计算\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 前向计算：将输入数据通过模型得到输出 logits\n",
    "        logits = model(datas)\n",
    "        # 计算当前 batch 的损失值，loss_fct 是预先定义的损失函数（例如交叉熵损失）\n",
    "        # logits 为模型预测的原始输出，labels 为真实标签\n",
    "        loss = loss_fct(logits, labels)         # 验证集损失\n",
    "        \n",
    "        # 将当前 batch 的损失值（通过 loss.item() 转换为 Python 数值）添加到 loss_list 中\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # 预测：对 logits 的最后一个维度（即各类别得分）取最大值的索引，\n",
    "        # 得到当前 batch 的预测类别标签\n",
    "        preds = logits.argmax(axis=-1)    # 验证集预测\n",
    "        \n",
    "        # 将预测的标签从 GPU（或当前设备）上移回 CPU，再转换为 numpy 数组，\n",
    "        # 最后转换为 Python 的 list，并将这些预测值扩展到 pred_list 中\n",
    "        pred_list.extend(preds.cpu().numpy().tolist())\n",
    "        # 同样的，将真实标签从设备移回 CPU，转换为 numpy 数组，再转换为 list，\n",
    "        # 将这些真实标签扩展到 label_list 中\n",
    "        label_list.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    # 使用 accuracy_score 函数计算所有 batch 累积的真实标签和预测标签之间的分类准确率\n",
    "    acc = accuracy_score(label_list, pred_list)\n",
    "    # 返回所有 batch 损失值的平均值（即验证集的平均损失）和计算得到的准确率\n",
    "    return np.mean(loss_list), acc\n"
   ],
   "id": "82205c15ce515207"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### TensorBoard 可视化\n",
    "\n",
    "\n",
    "训练过程中可以使用如下命令启动tensorboard服务。\n",
    "\n",
    "```shell\n",
    "tensorboard \\\n",
    "    --logdir=runs \\     # log 存放路径\n",
    "    --host 0.0.0.0 \\    # ip\n",
    "    --port 8848         # 端口\n",
    "```"
   ],
   "id": "737c134d5be236c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "class TensorBoardCallback:\n",
    "    def __init__(self, log_dir, flush_secs=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            log_dir (str): dir to write log.\n",
    "            flush_secs (int, optional): write to dsk each flush_secs seconds. Defaults to 10.\n",
    "        \"\"\"\n",
    "        self.writer = SummaryWriter(log_dir=log_dir, flush_secs=flush_secs)\n",
    "\n",
    "    def draw_model(self, model, input_shape):\n",
    "        self.writer.add_graph(model, input_to_model=torch.randn(input_shape))\n",
    "\n",
    "    def add_loss_scalars(self, step, loss, val_loss):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/loss\",\n",
    "            tag_scalar_dict={\"loss\": loss, \"val_loss\": val_loss},\n",
    "            global_step=step,\n",
    "            )\n",
    "\n",
    "    def add_acc_scalars(self, step, acc, val_acc):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/accuracy\",\n",
    "            tag_scalar_dict={\"accuracy\": acc, \"val_accuracy\": val_acc},\n",
    "            global_step=step,\n",
    "        )\n",
    "\n",
    "    def add_lr_scalars(self, step, learning_rate):\n",
    "        self.writer.add_scalars(\n",
    "            main_tag=\"training/learning_rate\",\n",
    "            tag_scalar_dict={\"learning_rate\": learning_rate},\n",
    "            global_step=step,\n",
    "\n",
    "        )\n",
    "\n",
    "    def __call__(self, step, **kwargs):\n",
    "        # add loss\n",
    "        loss = kwargs.pop(\"loss\", None)\n",
    "        val_loss = kwargs.pop(\"val_loss\", None)\n",
    "        if loss is not None and val_loss is not None:\n",
    "            self.add_loss_scalars(step, loss, val_loss)\n",
    "        # add acc\n",
    "        acc = kwargs.pop(\"acc\", None)\n",
    "        val_acc = kwargs.pop(\"val_acc\", None)\n",
    "        if acc is not None and val_acc is not None:\n",
    "            self.add_acc_scalars(step, acc, val_acc)\n",
    "        # add lr\n",
    "        learning_rate = kwargs.pop(\"lr\", None)\n",
    "        if learning_rate is not None:\n",
    "            self.add_lr_scalars(step, learning_rate)\n"
   ],
   "id": "367a1b729d3a16ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Best\n",
   "id": "a2d47703ea1cad3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SaveCheckpointsCallback:\n",
    "    def __init__(self, save_dir, save_step=5000, save_best_only=True):\n",
    "        \"\"\"\n",
    "        Save checkpoints each save_epoch epoch.\n",
    "        We save checkpoint by epoch in this implementation.\n",
    "        Usually, training scripts with pytorch evaluating model and save checkpoint by step.\n",
    "\n",
    "        Args:\n",
    "            save_dir (str): dir to save checkpoint\n",
    "            save_epoch (int, optional): the frequency to save checkpoint. Defaults to 1.\n",
    "            save_best_only (bool, optional): If True, only save the best model or save each model at every epoch.\n",
    "        \"\"\"\n",
    "        self.save_dir = save_dir\n",
    "        self.save_step = save_step\n",
    "        self.save_best_only = save_best_only\n",
    "        self.best_metrics = -1\n",
    "\n",
    "        # mkdir\n",
    "        if not os.path.exists(self.save_dir):\n",
    "            os.mkdir(self.save_dir)\n",
    "\n",
    "    def __call__(self, step, state_dict, metric=None):\n",
    "        if step % self.save_step > 0:\n",
    "            return\n",
    "\n",
    "        if self.save_best_only:\n",
    "            assert metric is not None\n",
    "            if metric >= self.best_metrics:\n",
    "                # save checkpoints\n",
    "                torch.save(state_dict, os.path.join(self.save_dir, \"best.ckpt\"))\n",
    "                # update best metrics\n",
    "                self.best_metrics = metric\n",
    "        else:\n",
    "            torch.save(state_dict, os.path.join(self.save_dir, f\"{step}.ckpt\"))\n",
    "\n"
   ],
   "id": "42fce4a198424560"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Early Stop",
   "id": "74a28b9b64c21449"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class EarlyStopCallback:\n",
    "    def __init__(self, patience=5, min_delta=0.01):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            patience (int, optional): Number of epochs with no improvement after which training will be stopped.. Defaults to 5.\n",
    "            min_delta (float, optional): Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute\n",
    "                change of less than min_delta, will count as no improvement. Defaults to 0.01.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_metric = -1\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, metric):\n",
    "        if metric >= self.best_metric + self.min_delta:\n",
    "            # update best metric\n",
    "            self.best_metric = metric\n",
    "            # reset counter\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "    @property\n",
    "    def early_stop(self):\n",
    "        return self.counter >= self.patience\n"
   ],
   "id": "6e95789c8fd5b95b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 训练\n",
    "def training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=None,\n",
    "    early_stop_callback=None,\n",
    "    eval_step=500,\n",
    "    ):\n",
    "    record_dict = {\n",
    "        \"train\": [],\n",
    "        \"val\": []\n",
    "    }\n",
    "\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    with tqdm(total=epoch * len(train_loader)) as pbar:\n",
    "        for epoch_id in range(epoch):\n",
    "            # training\n",
    "            for datas, labels in train_loader:\n",
    "                datas = datas.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # 梯度清空\n",
    "                optimizer.zero_grad()\n",
    "                # 模型前向计算\n",
    "                logits = model(datas)\n",
    "                # 计算损失\n",
    "                loss = loss_fct(logits, labels)\n",
    "                # 梯度回传\n",
    "                loss.backward()\n",
    "                # 调整优化器，包括学习率的变动等\n",
    "                optimizer.step()\n",
    "                preds = logits.argmax(axis=-1)\n",
    "\n",
    "                acc = accuracy_score(labels.cpu().numpy(), preds.cpu().numpy())\n",
    "                loss = loss.cpu().item()\n",
    "                # record\n",
    "\n",
    "                record_dict[\"train\"].append({\n",
    "                    \"loss\": loss, \"acc\": acc, \"step\": global_step\n",
    "                })\n",
    "\n",
    "                # evaluating\n",
    "                if global_step % eval_step == 0:\n",
    "                    model.eval()\n",
    "                    val_loss, val_acc = evaluating(model, val_loader, loss_fct)\n",
    "                    record_dict[\"val\"].append({\n",
    "                        \"loss\": val_loss, \"acc\": val_acc, \"step\": global_step\n",
    "                    })\n",
    "                    model.train()\n",
    "\n",
    "                    # 1. 使用 tensorboard 可视化\n",
    "                    if tensorboard_callback is not None:\n",
    "                        tensorboard_callback(\n",
    "                            global_step,\n",
    "                            loss=loss, val_loss=val_loss,\n",
    "                            acc=acc, val_acc=val_acc,\n",
    "                            lr=optimizer.param_groups[0][\"lr\"],\n",
    "                            )\n",
    "\n",
    "                    # 2. 保存模型权重 save model checkpoint\n",
    "                    if save_ckpt_callback is not None:\n",
    "                        save_ckpt_callback(global_step, model.state_dict(), metric=val_acc)\n",
    "\n",
    "                    # 3. 早停 Early Stop\n",
    "                    if early_stop_callback is not None:\n",
    "                        early_stop_callback(val_acc)\n",
    "                        if early_stop_callback.early_stop:\n",
    "                            print(f\"Early stop at epoch {epoch_id} / global_step {global_step}\")\n",
    "                            return record_dict\n",
    "\n",
    "                # udate step\n",
    "                global_step += 1\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"epoch\": epoch_id})\n",
    "\n",
    "    return record_dict\n",
    "\n",
    "\n",
    "epoch = 20\n",
    "\n",
    "activation = \"relu\"\n",
    "model = CNN(num_classes=10, activation=activation)\n",
    "\n",
    "# 1. 定义损失函数 采用交叉熵损失\n",
    "loss_fct = nn.CrossEntropyLoss()\n",
    "# 2. 定义优化器 采用 adam\n",
    "# Optimizers specified in the torch.optim package\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=1e-7)\n",
    "\n",
    "# 1. tensorboard 可视化\n",
    "if not os.path.exists(\"runs\"):\n",
    "    os.mkdir(\"runs\")\n",
    "tensorboard_callback = TensorBoardCallback(f\"runs/monkeys-cnn-{activation}\")\n",
    "tensorboard_callback.draw_model(model, [1, 3, img_h, img_w])\n",
    "# 2. save best\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "save_ckpt_callback = SaveCheckpointsCallback(f\"checkpoints/monkeys-cnn-{activation}\", save_step=len(train_loader), save_best_only=True)\n",
    "# 3. early stop\n",
    "early_stop_callback = EarlyStopCallback(patience=5)\n",
    "\n",
    "model = model.to(device)\n",
    "record = training(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epoch,\n",
    "    loss_fct,\n",
    "    optimizer,\n",
    "    tensorboard_callback=None,\n",
    "    save_ckpt_callback=save_ckpt_callback,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    eval_step=len(train_loader)\n",
    "    )"
   ],
   "id": "c9012f8c94df734"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#画线要注意的是损失是不一定在零到1之间的\n",
    "def plot_learning_curves(record_dict, sample_step=500):\n",
    "    # build DataFrame\n",
    "    train_df = pd.DataFrame(record_dict[\"train\"]).set_index(\"step\").iloc[::sample_step]\n",
    "    val_df = pd.DataFrame(record_dict[\"val\"]).set_index(\"step\")\n",
    "\n",
    "    # plot\n",
    "    fig_num = len(train_df.columns)\n",
    "    fig, axs = plt.subplots(1, fig_num, figsize=(5 * fig_num, 5))\n",
    "    for idx, item in enumerate(train_df.columns):\n",
    "        axs[idx].plot(train_df.index, train_df[item], label=f\"train_{item}\")\n",
    "        axs[idx].plot(val_df.index, val_df[item], label=f\"val_{item}\")\n",
    "        axs[idx].grid()\n",
    "        axs[idx].legend()\n",
    "        # axs[idx].set_xticks(range(0, train_df.index[-1], 5000))\n",
    "        # axs[idx].set_xticklabels(map(lambda x: f\"{int(x/1000)}k\", range(0, train_df.index[-1], 5000)))\n",
    "        axs[idx].set_xlabel(\"step\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(record, sample_step=10)  #横坐标是 steps"
   ],
   "id": "8ee11668e73543c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 评估",
   "id": "8a270d35f4ad0cb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# dataload for evaluating\n",
    "\n",
    "# load checkpoints\n",
    "model.load_state_dict(torch.load(f\"checkpoints/monkeys-cnn-{activation}/best.ckpt\", map_location=\"cpu\"))\n",
    "\n",
    "model.eval()\n",
    "loss, acc = evaluating(model, val_loader, loss_fct)\n",
    "print(f\"loss:     {loss:.4f}\\naccuracy: {acc:.4f}\")"
   ],
   "id": "de4c4ff957c69db9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
